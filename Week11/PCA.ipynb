{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docs for many methods are available here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
    "\n",
    "Some particular sub-links:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA \n",
    "\n",
    "\n",
    "In this notebook, I have a few tasks for you to try out related to PCA :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "#from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.datasets import make_s_curve\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# small sample data set and PCA process\n",
    "np.random.seed(42)\n",
    "\n",
    "covar = 8\n",
    "dat_low = np.random.multivariate_normal([100, 50], [[10, covar], [covar, 10]], 1000)\n",
    "\n",
    "print(dat_low.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "p = plt.subplot(1, 2, 1)\n",
    "p.scatter(dat_low[:, 0], dat_low[:, 1], c=[1 for i in range(dat_low.shape[0])], cmap=cm_bright)\n",
    "p.axis('equal')\n",
    "p.set_title(\"Sample Data\")\n",
    "\n",
    "n_comp = 2\n",
    "pca_mod = PCA(n_components = n_comp)\n",
    "pca_mod.fit(dat_low)\n",
    "new_pca_features = pca_mod.transform(dat_low)\n",
    "\n",
    "\n",
    "print('Explained variance ratio: \\n', pca_mod.explained_variance_ratio_ )\n",
    "print('Cumulative explained variance: \\n', np.cumsum(pca_mod.explained_variance_ratio_))\n",
    "print('PCA components: \\n', pca_mod.components_ )\n",
    "\n",
    "\n",
    "p = plt.subplot(1, 2, 2)\n",
    "p.scatter(new_pca_features[:, 0], new_pca_features[:, 1], \n",
    "          c=[1 for i in range(new_pca_features.shape[0])], cmap=cm_bright)\n",
    "p.axis('equal')\n",
    "p.set_title(\"Sample Data after PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: PCA -- visualizing PCA and choosing the number of components\n",
    "\n",
    "Some code is provided to generate correlated data in an arbitrary number of dimensions.\n",
    "\n",
    "NOTE: Do not set the dimensions for make_correlated_data to anything less than 3\n",
    "\n",
    "1. Use the independent data first. Plot the amount of explained variance vs the number of components. How can you use this chart to choose the number of componenets? Examine the function make_independent_data, look particularly at the covariance used to generate the data. Does this explain the PCA results?\n",
    "\n",
    "2. Examine the principle components themselves. How can you tell which original dimensions are being used in the componenets? What changes as you switch between independent data and correlated data? What happens as you increase phi from a low value (e.g. 0.1) to a high value (e.g 0.9) when you generate correlated data?\n",
    "\n",
    "3. Try the first question again with the correlated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "\n",
    "def make_independent_data(dimension=3, n=1000, version=1):\n",
    "    '''\n",
    "    Makes independent data in \"dimension\" dimensions\n",
    "    '''\n",
    "    if version == 1:\n",
    "        covar = np.diag([10 / float( (i + 1) ** 2) for i in range(dimension)])\n",
    "    else:\n",
    "        covar = np.diag([10 for i in range(dimension)])\n",
    "    print(\"Covariance matrix:\\n\", covar)\n",
    "    dat_i = np.random.multivariate_normal([0 for i in range(dimension)], covar, n)\n",
    "    \n",
    "    return dat_i\n",
    "\n",
    "\n",
    "def make_correlated_data(dimension, phi=0.5, n=1000):\n",
    "    '''\n",
    "    Makes dependent data in \"dimension\" dimensions, correlation between\n",
    "    index-adjacent dimensions controlled with \"phi\"\n",
    "    NOTE: Phi must be between 0 and 1\n",
    "    '''\n",
    "    \n",
    "    covar = np.zeros((dimension, dimension))\n",
    "    covar_vec = np.linspace(1,0,dimension)\n",
    "    for ii in range(dimension):\n",
    "        for jj in range(dimension):\n",
    "            covar[ii, jj] = np.power(phi, np.abs(ii - jj))\n",
    "            #covar[ii, jj] = covar_vec[np.abs(ii-jj)]\n",
    "    \n",
    "        dat = np.random.multivariate_normal([0 for i in range(dimension)], covar, n)\n",
    "        \n",
    "    return dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_comp = 5\n",
    "dat_independent = make_independent_data(n_comp)\n",
    "\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 5\n",
    "dat_independent = make_independent_data(n_comp, version=2)\n",
    "\n",
    "## YOUR CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 5\n",
    "dat_corr = make_correlated_data(n_comp, phi=0.9, n=1000)\n",
    "                                  \n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Non-linear Methods\n",
    "\n",
    "1. Use the circles data. Plot the data, fit regular PCA to the data. Is this very effective? Why or why not?\n",
    "\n",
    "2. Now try using KernelPCA instead. This is a little trickier than regular PCA; follow these steps:\n",
    "\n",
    "2a: Fit Kernel PCA to the circles data using KernelPCA, try 5 components to start. Use the rbf kernel, with gamma = 1 (see the arguments in the documentation).\n",
    "\n",
    "2b: The lambda\\_ values roughly correspond to the amount of explained variance. You'll need to normalize by their sum to get an idea of how much variance each component explains. How much variance do the number of componenets explain? Try adding more than 5 componenets: do these extra components add any value? Explore the resulting data by plotting 2D projections. Which component of the resulting data would be a good projection to 1D?\n",
    "\n",
    "2c: It is not easy to see exactly what each component means in the original space, but we can look at the data points projected into the kernel PCA space. Use the alpha\\_ values to see this. Make a plot for both the inner and outer circle. Do you see any patterns?\n",
    "\n",
    "If you have time, try this process on:\n",
    "\n",
    "swiss roll data\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html\n",
    "\n",
    "s curve data\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_s_curve.html#sklearn.datasets.make_s_curve\n",
    "\n",
    "these don't have two pieces like the circle data, but have a simple nonlinear structure that can be captured with kernel PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# make some circle data:\n",
    "# dat_circles is the raw data, dat_circle label is a label indicating\n",
    "# if the point is in the inner or outer circle (see the plot below)\n",
    "dat_circles, dat_circle_label = make_circles(250, noise=0.01, factor=0.25)\n",
    "\n",
    "# simple scatterplot of the data, so you get the idea ;)\n",
    "plt.scatter(dat_circles[:, 0], dat_circles[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
