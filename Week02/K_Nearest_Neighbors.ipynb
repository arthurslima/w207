{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and test a Nearest Neighbors classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Iris data to use for experiments. The data include 50 observations of each of 3 types of irises (150 total). Each observation includes 4 measurements: sepal and petal width and height. The goal is to predict the iris type from these measurements.\n",
    "\n",
    "<http://en.wikipedia.org/wiki/Iris_flower_data_set>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, which is included in sklearn.\n",
    "iris = load_iris()\n",
    "print('Iris target names:', iris.target_names)\n",
    "print('Iris feature names:', iris.feature_names)\n",
    "X, Y = iris.data, iris.target\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "train_data, train_labels = X[:100], Y[:100]\n",
    "test_data, test_labels = X[100:], Y[100:]\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a distance function that returns the distance between 2 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance(v1, v2):\n",
    "    sum = 0.0\n",
    "    for index in range(len(v1)):\n",
    "        sum += (v1[index] - v2[index]) ** 2\n",
    "    return sum ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's compute all the pairwise distances in the training data and plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "for i in range(len(train_data) - 1):\n",
    "    for j in range(i + 1, len(train_data)):\n",
    "        dist = EuclideanDistance(train_data[i], train_data[j])\n",
    "        dists.append(dist)\n",
    "        \n",
    "fig = plt.hist(dists, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now let's create a class that implements a Nearest Neighbors classifier. We'll model it after the sklearn classifier implementations, with fit() and predict() methods.\n",
    "\n",
    "<http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighbors:\n",
    "    # Initialize an instance of the class.\n",
    "    def __init__(self, metric=EuclideanDistance):\n",
    "        self.metric = metric\n",
    "    \n",
    "    # No training for Nearest Neighbors. Just store the data.\n",
    "    def fit(self, train_data, train_labels):\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "    \n",
    "    # Make predictions for each test example and return results.\n",
    "    def predict(self, test_data):\n",
    "        results = []\n",
    "        for item in test_data:\n",
    "            results.append(self._predict_item(item))\n",
    "        return results\n",
    "    \n",
    "    # Private function for making a single prediction.\n",
    "    def _predict_item(self, item):\n",
    "        best_dist, best_label = 1.0e10, None\n",
    "        for i in range(len(self.train_data)):\n",
    "            dist = self.metric(self.train_data[i], item)\n",
    "            if dist < best_dist:\n",
    "                best_label = self.train_labels[i]\n",
    "                best_dist = dist\n",
    "        return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an experiment with the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NearestNeighbors()\n",
    "clf.fit(train_data, train_labels)\n",
    "preds = clf.predict(test_data)\n",
    "\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, test_labels):\n",
    "    if pred == label: correct += 1\n",
    "    total += 1\n",
    "print('total: %3d  correct: %3d  accuracy: %3.2f' %(total, correct, 1.0*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k nearest neighbors\n",
    "\n",
    "The implementation above only allows for a single nearest neighbor; that is, the classifier predicts the label of the closest available point. What about using more than one nearest neighbor. Typically, this means to make a prediciton we:\n",
    "\n",
    "1. Find the k closest points (according to our distance metric) to the query point.\n",
    "2. Find the majority label of those k points found in (1)\n",
    "3. Return the label in (2) as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurKNearestNeighbors:\n",
    "    # Initialize an instance of the class.\n",
    "    def __init__(self, metric=EuclideanDistance):\n",
    "        self.metric = metric\n",
    "    \n",
    "    # No training for Nearest Neighbors. Just store the data.\n",
    "    def fit(self, train_data, train_labels):\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "    \n",
    "    # Make predictions for each test example and return results.\n",
    "    def predict(self, test_data):\n",
    "        results = []\n",
    "        for item in test_data:\n",
    "            results.append(self._predict_item(item))\n",
    "        return results\n",
    "    \n",
    "    # Private function for making a single prediction using KNN.\n",
    "    def _predict_item(self, item):\n",
    "        \n",
    "        #### YOUR CODE HERE\n",
    "        \n",
    "        best_label = -1\n",
    "        return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking k: the number of neighbors to use in classification\n",
    "\n",
    "If you did not do the above, you can use sklearn's nearest neighbors classifier for an implementation with multiple k\n",
    "\n",
    "Implement a way to pick the number of neighbors to use in the classifier. We already have a test set, so simply extend the procedure in the previous code cell to run over different numbers of neighbors. Plot the test set performance versus the number of neighbors.\n",
    "\n",
    "(note: you can use sklearn's implementation here; for a challenge, implement knn above [only 1-nn is implemented right now!])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results\n",
    "\n",
    "We've been a litte haphazard so far, we should have plotted the data and some results to get an idea of how the algorithm is performing. Plot the data with the true labels as colors, and plot it with some predicted labels as colors, for differing values of k, to see how our KNN algorithm is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "\n",
    "#### YOUR CODE HERE\n",
    "\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF', '#00FF00'])\n",
    "p = plt.subplot(1, 2, 1)\n",
    "p.scatter(test_data[:, 0], test_data[:, 1], c=test_labels, cmap=cm_bright)\n",
    "plt.title(\"Iris Test Data: X1 vs X2\")\n",
    "\n",
    "p = plt.subplot(1, 2, 2)\n",
    "p.scatter(test_data[:, 2], test_data[:, 3], c=test_labels, cmap=cm_bright)\n",
    "plt.title(\"Iris Test Data: X3 vs X4\")\n",
    "plt.show()\n",
    "\n",
    "#### YOUR CODE HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A different distance metric\n",
    "\n",
    "We used the euclidean distance metric, try implementing another one. A couple of suggestions:\n",
    "\n",
    "1. L1 (manhattan) distance\n",
    "2. Lp distance ( https://en.wikipedia.org/wiki/Lp_space )\n",
    "3. L-infinity distance\n",
    "4. Try a metric that re-weights by dimension index, e.g. discard one dimension entirely\n",
    "\n",
    "Do any of these improve or greatly affect the results? (Use the test set to back up any statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "# a reminder of what euclidean distance looks like\n",
    "def EuclideanDistance(v1, v2):\n",
    "    sum = 0.0\n",
    "    for index in range(len(v1)):\n",
    "        sum += (v1[index] - v2[index]) ** 2\n",
    "    return sum ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
