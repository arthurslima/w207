{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Interest in neural networks, and in particular those with architechures that support deep learning, has been surging in recent years.\n",
    "\n",
    "In this notebook we will be revisiting the problem of digit classification on the MNIST data. In doing so, we will introduce a new Machine Learning library, Tensorflow, for working with neural networks. \n",
    "\n",
    "Some words to TensorFlow...\n",
    "\n",
    "In part 1, we'll introduce Tensorflow, and refresh ourselves on the MNIST dataset. In part 2, we'll create a multi-layer neural network with a simple architechure, and train it using backpropagation. Part 3 will introduce the convolutional architechure, which can be said to be doing 'deep learning' (also called feature learning or representation learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from IPython.display import display, clear_output \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X / 255.0\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "numExamples = 2000\n",
    "test_data, test_labels = X[70000-numExamples:], Y[70000-numExamples:]\n",
    "train_data, train_labels = X[:numExamples], Y[:numExamples]\n",
    "numFeatures = train_data[1].size\n",
    "numClasses = 10\n",
    "numSamples = train_data.shape[0]\n",
    "numTestExamples = test_data.shape[0]\n",
    "print('Features = %d' %(numFeatures))\n",
    "print('Train set = %d' %(numSamples))\n",
    "print('Test set = %d' %(numTestExamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 2: Multi-layer Neural Networks\n",
    "\n",
    "As we start to get further into Neural Networks, if you'd like to take time on your own for an in-depth introduction on the state of the art in the topic, check out this excellent 1-day tutorial from KDD2014:\n",
    "\n",
    "Part 1: http://videolectures.net/kdd2014_bengio_deep_learning/\n",
    "\n",
    "Part 2: http://videolectures.net/tcmm2014_taylor_deep_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "testX = tf.constant(test_data, dtype = tf.float32)\n",
    "hiddenlayer_size = 80\n",
    "miniBatchSize = 10;\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w1 = tf.get_variable('w1', shape=[numFeatures, hiddenlayer_size])\n",
    "b1 = tf.get_variable('b1', shape=[hiddenlayer_size])\n",
    "w2 = tf.get_variable('w2', shape=[hiddenlayer_size, numClasses])\n",
    "b2 = tf.get_variable('b2', shape=[numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer):\n",
    "    hidden_layer = tf.nn.sigmoid(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer, w2) + b2)\n",
    "    return output_layer\n",
    "\n",
    "# (3) Cost\n",
    "def cost(data, labels):\n",
    "    cc = tf.losses.log_loss(labels, model(data))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    for i in range(20):\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data[start:end], train_labels[start:end]\n",
    "            _, cost, test__preds = sess.run([step, cc, test_preds], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "        cost_vec.append(cost)\n",
    "        clear_output(wait=True)\n",
    "        print('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test__preds, axis=1).astype(str) == test_labels)))\n",
    "    \n",
    "plt.plot(cost_vec)  \n",
    "plt.xlabel('Number of Gradient Descent steps')\n",
    "plt.ylabel('Cost function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exercise: Change the number of nodes in the hidden layer?  What do you expect the impact to be?  What is the impact?\n",
    "\n",
    "\n",
    "As interest in networks with more layers and more complicated architechures has increased, a couple of tricks have emerged and become standard practice.  Let's look at two of those--rectifier activation and dropout noise.\n",
    "\n",
    "Exercise:  We saw an improvement from adding a hidden layer.  What do you expect to happen if a second hidden layer was added?  \n",
    "\n",
    "Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Activation Revisted\n",
    "\n",
    "Let's look at a recent idea around activation closely associated with deep learning.  In 2010, in a paper published at NIPS (https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf), Yoshua Bengio showed that rectifier activation works better empirically than sigmoid activation when used in the hidden layers.  \n",
    "\n",
    "The rectifier activation is simple: f(x)=max(0,x).  Intuitively, the difference is that as a sigmoid activated node approaches 1 it stops learning even if error continues to be propagated to it, whereas the rectifier activated node continue to learn (at least in the positive direction).  It is not completely understood (per Yoshua Bengio) why this helps, but there are some theories being explored including as related to the benefits of sparse representations in networks. (http://www.iro.umontreal.ca/~bengioy/talks/KDD2014-tutorial.pdf).  Rectifiers also speed up training.\n",
    "\n",
    "Although the paper was published in 2010, the technique didn't gain widespread adoption until 2012 when members of Hinton's group spread the word, including with this Kaggle entry: http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/\n",
    "\n",
    "In TensorFlow the rectifier activation is implemented by the function tf.nn.relu().\n",
    "Let's change the activation in our 2 layer network to rectifier and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Maxout Activation\n",
    "\n",
    "So rectifier activation worked great!  \n",
    "\n",
    "Exercise: try another type of activation called Maxout (or Max Pooling) activiation.  Maxout activation just selects the max input as the output.  Maxout is a type of pooling, a technique which performs particularly well for vision problems. For more background see: http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf and http://www.quora.com/What-is-impact-of-different-pooling-methods-in-convolutional-neural-networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Noise\n",
    "\n",
    "Previously when working with the MNIST data we saw a benefit in generalization from adding noise to the training data.  Let's try that again here, however this time with a trick for adding noise called 'Dropouts'.  The idea with dropouts is that instead of (or in addition to) adding noise to our inputs, we add noise by having each node return 0 with a certain probability during training.  This trick both improves generalization in large networks and speeds up training.\n",
    "\n",
    "Hinton introduced the idea in 2012 and gave an explanation of why it's similar to bagging (http://arxiv.org/pdf/1207.0580v1.pdf)\n",
    "\n",
    "Let's give it a try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "testX = tf.constant(test_data, dtype = tf.float32)\n",
    "hiddenlayer_size = 120\n",
    "p_keep1 = 0.8\n",
    "p_keep2 = 0.6\n",
    "miniBatchSize = 10;\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w1 = tf.get_variable('w1', shape=[numFeatures, hiddenlayer_size])\n",
    "b1 = tf.get_variable('b1', shape=[hiddenlayer_size])\n",
    "w2 = tf.get_variable('w2', shape=[hiddenlayer_size, numClasses])\n",
    "b2 = tf.get_variable('b2', shape=[numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer, is_training = True):\n",
    "    if is_training:\n",
    "        pk1 = p_keep1\n",
    "        pk2 = p_keep2\n",
    "    else:\n",
    "        pk1 = 1\n",
    "        pk2 = 1\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf.nn.dropout(input_layer, pk1), w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(tf.nn.dropout(hidden_layer, pk2), w2) + b2)\n",
    "    return output_layer\n",
    "\n",
    "# (3) Cost\n",
    "def cost(data, labels):\n",
    "    cc = tf.losses.log_loss(labels, model(data))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX, is_training = False)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    for i in range(10):\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data[start:end], train_labels[start:end]\n",
    "            _, cost, test__preds = sess.run([step, cc, test_preds], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "        cost_vec.append(cost)\n",
    "        clear_output(wait=True)\n",
    "        print('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test__preds, axis=1).astype(str) == test_labels)))\n",
    "    \n",
    "plt.plot(cost_vec)  \n",
    "plt.xlabel('Number of Gradient Descent steps')\n",
    "plt.ylabel('Cost function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part 3: Convolutional Neural Nets\n",
    "\n",
    "Today, when the phrase 'deep learning' is used to describe a system, it is often a convolution neural network (or convonet).  The convonet architechture was largely developed in the late 90's at Bell Labs, but only very recently popularized.  It was developed for image recognition, and is described and implemented with 2d representations in mind.\n",
    "\n",
    "Geoffrey Hinton has an excellent two-part lecture on the topic:\n",
    "\n",
    "https://www.youtube.com/watch?v=6oD3t6u5EPs\n",
    "\n",
    "https://www.youtube.com/watch?v=fueIAeAsGzA\n",
    "\n",
    "Also, this code was partly taken from these tutorials, which are worth referring back to:\n",
    "\n",
    "http://deeplearning.net/tutorial/lenet.html\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/\n",
    "\n",
    "https://www.youtube.com/watch?v=S75EdAcXHKk\n",
    "\n",
    "http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "image_size = int(np.sqrt(numFeatures))   # numFeatures should be a square number\n",
    "p_keep1 = 0.8\n",
    "p_keep2 = 0.6\n",
    "p_keep3 = 0.6\n",
    "hiddenlayer_size = 120\n",
    "filter_size = 5\n",
    "depth_size = 3\n",
    "mp_size = 2\n",
    "miniBatchSize = 20;\n",
    "testX = tf.constant(test_data, dtype = tf.float32)\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w1 = tf.get_variable('w1', shape=[filter_size, filter_size, 1, depth_size])\n",
    "w2 = tf.get_variable('w2', shape=[int(image_size/mp_size*image_size/mp_size*depth_size), hiddenlayer_size])\n",
    "b2 = tf.get_variable('b2', shape=[hiddenlayer_size])\n",
    "w3 = tf.get_variable('w3', shape=[hiddenlayer_size, numClasses])\n",
    "b3 = tf.get_variable('b3', shape=[numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer, is_training = True):\n",
    "    if is_training:\n",
    "        pk1 = p_keep1\n",
    "        pk2 = p_keep2\n",
    "        pk3 = p_keep3\n",
    "    else:\n",
    "        pk1 = 1\n",
    "        pk2 = 1\n",
    "        pk3 = 1\n",
    "        \n",
    "    l1 = tf.reshape(input_layer, [-1, image_size, image_size, 1])\n",
    "    l2 = tf.nn.dropout(l1, pk1)\n",
    "    l3 = tf.nn.conv2d(l2, w1, [1,1,1,1], 'SAME')\n",
    "    l4 = tf.nn.relu(l3)\n",
    "    l5 = tf.nn.max_pool(l4, [1,mp_size, mp_size, 1], [1, mp_size, mp_size,1], 'VALID')\n",
    "    \n",
    "    l6 = tf.reshape(l5, [-1, int(image_size/mp_size*image_size/mp_size*depth_size)])\n",
    "    l7 = tf.nn.dropout(l6, pk2)\n",
    "    l8 = tf.matmul(l7, w2) + b2\n",
    "    l9 = tf.nn.relu(l8)\n",
    "    \n",
    "    l10 = tf.nn.dropout(l9, pk3)\n",
    "    l11 = tf.matmul(l10, w3) + b3\n",
    "    output_layer = tf.nn.softmax(l11)\n",
    "    return output_layer\n",
    "\n",
    "# (3) Cost\n",
    "def cost(data, labels):\n",
    "    cc = tf.losses.log_loss(labels, model(data))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX, is_training = False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    for i in range(5):\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data[start:end], train_labels[start:end]\n",
    "            _, cost, test__preds = sess.run([step, cc, test_preds], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "        cost_vec.append(cost)\n",
    "        clear_output(wait=True)\n",
    "        print('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test__preds, axis=1).astype(str) == test_labels)))\n",
    "    \n",
    "plt.plot(cost_vec)  \n",
    "plt.xlabel('Number of Gradient Descent steps')\n",
    "plt.ylabel('Cost function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last cell we use the predefined layers provided by Tensorflow. In this case, we do not need to define the variables by ourselves, but TensorFlow creates the variables for us.\n",
    "Check this out:\n",
    "https://www.tensorflow.org/tutorials/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "image_size = int(np.sqrt(numFeatures))   # numFeatures should be a square number\n",
    "p_keep1 = 0.8\n",
    "p_keep2 = 0.6\n",
    "p_keep3 = 0.6\n",
    "hiddenlayer_size = 120\n",
    "filter_size = 5\n",
    "depth_size = 3\n",
    "mp_size = 2\n",
    "miniBatchSize = 20\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "is_training_ = tf.placeholder_with_default(True, shape=[], name='is_training')\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer, is_training):\n",
    "    l1 = tf.reshape(input_layer, [-1, image_size, image_size, 1])\n",
    "    l2 = tf.layers.dropout(inputs = l1,\n",
    "                          rate = 1-p_keep1,\n",
    "                          training = is_training)\n",
    "    l3 = tf.layers.conv2d(inputs = l2, \n",
    "                         filters = depth_size, \n",
    "                         kernel_size = [filter_size, filter_size], \n",
    "                         padding='same', \n",
    "                         activation=tf.nn.relu)\n",
    "    l4 = tf.layers.max_pooling2d(inputs = l3, \n",
    "                                pool_size = [mp_size, mp_size], \n",
    "                                strides = mp_size)\n",
    "    l5 = tf.reshape(l4, [-1, int(image_size/mp_size * image_size/mp_size * depth_size)])\n",
    "    l6 = tf.layers.dropout(inputs = l5,\n",
    "                          rate = 1-p_keep2,\n",
    "                          training = is_training)\n",
    "    l7 = tf.layers.dense(inputs = l6, \n",
    "                        units = hiddenlayer_size,\n",
    "                        activation = tf.nn.relu)\n",
    "    l8 = tf.layers.dropout(inputs = l7,\n",
    "                          rate = 1-p_keep2,\n",
    "                          training = is_training)\n",
    "    l9 = tf.layers.dense(inputs = l8, \n",
    "                        units = hiddenlayer_size,\n",
    "                        activation = tf.nn.relu)\n",
    "    l10 = tf.layers.dropout(inputs = l9,\n",
    "                          rate = 1-p_keep3,\n",
    "                          training = is_training)\n",
    "    output_layer = tf.layers.dense(inputs = l10,\n",
    "                        units = numClasses,\n",
    "                        activation = tf.nn.softmax)\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "preds = model(x_, is_training_)\n",
    "cc = tf.losses.log_loss(y_one_hot, preds)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "\n",
    "# (5) Use model to calculate accuracy\n",
    "acc = tf.metrics.accuracy(y_, tf.argmax(preds, axis=1))[1]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    cost_vec = []\n",
    "    acc_test_vec = []\n",
    "    acc_train_vec = []\n",
    "    for i in range(40):\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data[start:end], train_labels[start:end]\n",
    "            _, cost = sess.run([step, cc], feed_dict={x_: batch[0], y_: batch[1], is_training_: True})\n",
    "            \n",
    "        test_acc = acc.eval(feed_dict={x_: test_data, y_: test_labels, is_training_: False})\n",
    "        train_acc = acc.eval(feed_dict={x_: train_data, y_: train_labels, is_training_: False})\n",
    "              \n",
    "        cost_vec.append(cost)\n",
    "        clear_output(wait=True)\n",
    "        acc_test_vec.append(test_acc)\n",
    "        acc_train_vec.append(train_acc)\n",
    "        print('%d) accuracy = %.4f' %(i+1, test_acc))\n",
    "    \n",
    "plt.plot(cost_vec)  \n",
    "plt.xlabel('Number of Gradient Descent steps')\n",
    "plt.ylabel('Cost function')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acc_test_vec)  \n",
    "plt.plot(acc_train_vec)\n",
    "plt.xlabel('Number of Gradient Descent steps')\n",
    "plt.ylabel('Train and Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
