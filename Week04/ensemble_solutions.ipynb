{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods: Tree Bagging; Random Forests; Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We talked a bit in passing about a few ensemble methods when we talked about trees etc. Let's take some time to use them! We'll go over both the sklearn implementations, and try implementing both ourselves. In the 'do it yourself' part, I'll give you a single iteration, it is your job to put it together ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll use some simulated data: concentric spheres of classes, see plots and examples here:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=4000, n_features=10,\n",
    "                                 n_classes=2, random_state=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "train_data, train_labels = X[:2000], Y[:2000]\n",
    "test_data, test_labels = X[2000:], Y[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore what sklearn has in terms of ensemble methods. There are two interesting ones we can use right now, adaboost and random forests. We'll start by using the sklearn ones, then try implementing random forests ourselves!\n",
    "\n",
    "Be sure to reference the documentation at:  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Let's start with just executing some sklearn functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (a decision tree): 0.759\n",
      "Accuracy (a random forest): 0.87\n",
      "Accuracy (adaboost with decision trees): 0.824\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "dt.fit(train_data, train_labels)\n",
    "\n",
    "print('Accuracy (a decision tree):', dt.score(test_data, test_labels))\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(train_data, train_labels)\n",
    "\n",
    "print('Accuracy (a random forest):', rfc.score(test_data, test_labels))\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "abc.fit(train_data, train_labels)\n",
    "print('Accuracy (adaboost with decision trees):', abc.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like ensemble methods do well, both do better than a single tree. Before moving on, try playing arond with some of the parameters, such as:\n",
    "\n",
    "n_estimators in RandomForestClassifier\n",
    "\n",
    "\n",
    "n_estimators and learning_rate AdaBoostClassifier\n",
    "\n",
    "Why do the methods behave as they when you tweak the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we consider the more widely usedRandom forests, which are combinations of many decision trees, let's start with a slightly simplified version: **tree bagging**. Here is a simple algorithm for tree bagging:\n",
    "\n",
    "1. Set B (number of trees to make)\n",
    "2. Repeat B times:\n",
    "  1. Draw N random samples from training data, with replacement, where N is the number of training data points\n",
    "  2. Fit a decision tree to this re-sampled data\n",
    "  3. Store the predictions from this decision tree on the test data\n",
    "3. As the final predictions on the test data, use the majority vote classification for the predictions above\n",
    "\n",
    "Below, I've given you an implementation of a single iteration of the main loop above. Complete the algorthim by (1) adding the repeated B resampling and fitting (2) implementing step 3 above, the final predictions from tree bagging.\n",
    "\n",
    "Once you've done that, does bagging do better than a single tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8835"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# a single iteration of tree bagging\n",
    "B = 500\n",
    "n = train_data.shape[0]\n",
    "sn = int(n*2.0/3.0)   # nr of training data in subset for each tree\n",
    "nf = train_data.shape[1]\n",
    "all_preds = np.zeros((B,test_data.shape[0]))\n",
    "\n",
    "for b in range(B):\n",
    "    bs_sample_index = np.random.choice(range(n), size=sn, replace=True)\n",
    "\n",
    "    bs_data = train_data[bs_sample_index, :]\n",
    "    bs_labels = train_labels[bs_sample_index]\n",
    "    bs_test_data = test_data\n",
    "    \n",
    "    bs_tree = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\")\n",
    "    bs_tree.fit(bs_data, bs_labels)\n",
    "    \n",
    "    bs_tree_preds = bs_tree.predict(bs_test_data)\n",
    "    all_preds[b,:] = bs_tree_preds\n",
    "    \n",
    "voting = np.sum(all_preds,axis=0) / B\n",
    "voting = [int(x >= 0.5) for x in voting]\n",
    "np.mean(voting==test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to do **random forests**. Random forests add the twist of subsampling features at each node. Typically, we take p' = sqrt(p) features. DecisionTreeClassifier implements with through the *max_features*, check out the documentation. A simple change to your above code should give you random forests.\n",
    "\n",
    "1. Set B (number of trees to make)\n",
    "2. Repeat B times:\n",
    "  1. Draw N random samples from training data, with replacement, where N is the number of training data points\n",
    "  2. Draw p' = sqrt(p) features without replacement\n",
    "  3. Fit a decision tree to this re-sampled data\n",
    "  4. Store the predictions from this decision tree on the test data\n",
    "3. As the final predictions on the test data, use the majority vote classification for the predictions above\n",
    "\n",
    "Does random forests do better than tree bagging?\n",
    "\n",
    "Note: you can also use trees, tree bagging, and random forests for regression! Now, the original data is a regression problem so just reload the data, and to do all of these ideas using trees, you need only use DecisionTreeRegressor instead of DecisionTreeClassifier; see:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "As a bonus, try implementing trees, tree bagging, and random forests for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.872"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# a single iteration of tree bagging\n",
    "B = 500\n",
    "n = train_data.shape[0]\n",
    "sn = int(n*2.0/3.0)   # nr of training data in subset for each tree\n",
    "nf = train_data.shape[1]\n",
    "all_preds = np.zeros((B,test_data.shape[0]))\n",
    "\n",
    "for b in range(B):\n",
    "    bs_sample_index = np.random.choice(range(n), size=sn, replace=True)\n",
    "    \n",
    "    bs_data = train_data[bs_sample_index, :]\n",
    "    bs_labels = train_labels[bs_sample_index]\n",
    "    bs_test_data = test_data\n",
    "\n",
    "    bs_sample_index_features = np.random.choice(range(nf), size=int(np.sqrt(nf)), replace=False)\n",
    "    bs_data = bs_data[:, bs_sample_index_features]\n",
    "    bs_test_data = bs_test_data[:, bs_sample_index_features]\n",
    "    \n",
    "    bs_tree = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\")\n",
    "    bs_tree.fit(bs_data, bs_labels)\n",
    "    \n",
    "    bs_tree_preds = bs_tree.predict(bs_test_data)\n",
    "    all_preds[b,:] = bs_tree_preds\n",
    "    \n",
    "voting = np.sum(all_preds,axis=0) / B\n",
    "voting = [int(x >= 0.5) for x in voting]\n",
    "np.mean(voting==test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
